{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: hello world -> Translated: hola mundo\n",
      "Original: good morning -> Translated: bueno mañana\n",
      "Original: how are you -> Translated: cómo estás tú\n",
      "Original: this is a test -> Translated: esto es un prueba\n"
     ]
    }
   ],
   "source": [
    "# 1 Implement a basic Statistical Machine Translation (SMT) model that uses word-by-word translation with a\n",
    "# dictionary lookup approachV\n",
    "import json\n",
    "\n",
    "def load_translation_dictionary():\n",
    "    \"\"\"Loads a simple word-to-word translation dictionary.\"\"\"\n",
    "    return {\n",
    "        \"hello\": \"hola\",\n",
    "        \"world\": \"mundo\",\n",
    "        \"good\": \"bueno\",\n",
    "        \"morning\": \"mañana\",\n",
    "        \"how\": \"cómo\",\n",
    "        \"are\": \"estás\",\n",
    "        \"you\": \"tú\",\n",
    "        \"is\": \"es\",\n",
    "        \"this\": \"esto\",\n",
    "        \"a\": \"un\",\n",
    "        \"test\": \"prueba\"\n",
    "    }\n",
    "\n",
    "def translate_sentence(sentence, translation_dict):\n",
    "    \"\"\"Translates a sentence using word-by-word dictionary lookup.\"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    translated_words = [translation_dict.get(word, word) for word in words]  # Keep original word if not found\n",
    "    return \" \".join(translated_words)\n",
    "\n",
    "# Load dictionary\n",
    "translation_dict = load_translation_dictionary()\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"hello world\",\n",
    "    \"good morning\",\n",
    "    \"how are you\",\n",
    "    \"this is a test\"\n",
    "]\n",
    "\n",
    "# Perform translation\n",
    "for sentence in sentences:\n",
    "    translated = translate_sentence(sentence, translation_dict)\n",
    "    print(f\"Original: {sentence} -> Translated: {translated}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m model \u001b[38;5;241m=\u001b[39m NMTModel(input_dim, hidden_dim, output_dim)\n\u001b[0;32m     41\u001b[0m src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, input_dim)  \u001b[38;5;66;03m# Batch of 5 sentences, 10 words each\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Placeholder tgt input\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m, in \u001b[0;36mNMTModel.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     27\u001b[0m encoder_outputs, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src)\n\u001b[0;32m     28\u001b[0m hidden \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mview(hidden\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Adjusting hidden shape\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m context_vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attn_weights\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), encoder_outputs)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m decoder_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(context_vector\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden, encoder_outputs):\n\u001b[0;32m     12\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[43mhidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m     energy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(torch\u001b[38;5;241m.\u001b[39mcat((hidden, encoder_outputs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m     15\u001b[0m     attention_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv(energy)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.permute(1, 0, 2).expand(-1, seq_len, -1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention_weights = F.softmax(self.v(energy).squeeze(2), dim=1)\n",
    "        return attention_weights\n",
    "\n",
    "class NMTModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NMTModel, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "        hidden = hidden.view(hidden.shape[1], -1, hidden.shape[2]).sum(dim=0)  # Adjusting hidden shape\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        decoder_output, _ = self.decoder(context_vector.unsqueeze(1))\n",
    "        output = self.fc(decoder_output.squeeze(1))\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "input_dim = 10\n",
    "hidden_dim = 128\n",
    "output_dim = 10\n",
    "model = NMTModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "src = torch.rand(5, 10, input_dim)  # Batch of 5 sentences, 10 words each\n",
    "output = model(src, src)  # Placeholder tgt input\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8559b07f17964ceb9ab316cfb9603ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vinay\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3571d43cff2d4a57aa1bcd561781ecd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025bbd9e4f5547f18551ebf1f7f13027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a148c2fe5d24a248818de27a17eac07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fde3176db4a422491eaaa81d7ef5992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa7534b3c5e4a81b6599510d9ca298d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d05d367cd14fe290e6c028be247959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour, comment allez-vous?', \"C'est un exemple de traduction automatique.\"]\n"
     ]
    }
   ],
   "source": [
    "# 4. Use a pre-trained GPT model to perform machine translation from English to French\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def translate(text, model_name=\"Helsinki-NLP/opus-mt-en-fr\"):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = model.generate(**inputs)\n",
    "    translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "english_text = [\"Hello, how are you?\", \"This is a machine translation example.\"]\n",
    "french_translation = translate(english_text)\n",
    "print(french_translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\vinay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\diamondpriceprediction-0.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c3895ee40e476bb49cd6c6000e971b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vinay\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c41572e41a43bb964602d58336764e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f96f0967744b2682dfb36eb38a8f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fbb9a264d64acdad0fa4fd46ea0f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47ec65975ac44a48f662b986f914fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211744a20490432eac994bbe50108811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba2647a98c24ce7938a04bba52694f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short poem about Nature:\n",
      "\n",
      "The poem is a poem of the nature of Nature.\n",
      "...\n",
      " (The poet is the poet, and the poem the writer.)\n",
      ",. (the poet and writer. The poem and\n"
     ]
    }
   ],
   "source": [
    "# 6. Generate a short poem using GPT-2 for a specific theme (e.g., \"Nature\")\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def generate_poem(theme, model_name=\"gpt2\"):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    input_text = f\"Write a short poem about {theme}:\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model.generate(inputs, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    poem = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return poem\n",
    "\n",
    "theme = \"Nature\"\n",
    "poem = generate_poem(theme)\n",
    "print(poem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 8.0, Reward: -8\n",
      "Epoch 10, Loss: 10.0, Reward: -10\n",
      "Epoch 20, Loss: 10.0, Reward: -10\n",
      "Epoch 30, Loss: 10.0, Reward: -10\n",
      "Epoch 40, Loss: 10.0, Reward: -10\n",
      "Epoch 50, Loss: 10.0, Reward: -10\n",
      "Epoch 60, Loss: 10.0, Reward: -10\n",
      "Epoch 70, Loss: 10.0, Reward: -10\n",
      "Epoch 80, Loss: 10.0, Reward: -10\n",
      "Epoch 90, Loss: 10.0, Reward: -10\n"
     ]
    }
   ],
   "source": [
    "# 7. Create a simple multimodal generative model that generates an image caption given an image.\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.rnn = nn.RNN(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "def generate_text(model, start_token, max_length, vocab_size):\n",
    "    model.eval()\n",
    "    input = torch.tensor([[start_token]], dtype=torch.long)\n",
    "    hidden = torch.zeros(1, 1, model.hidden_dim)\n",
    "    generated_text = [start_token]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        output, hidden = model(input, hidden)\n",
    "        probs = F.softmax(output[0, -1], dim=0)\n",
    "        next_token = torch.multinomial(probs, 1).item()\n",
    "        generated_text.append(next_token)\n",
    "        input = torch.tensor([[next_token]], dtype=torch.long)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def compute_reward(generated_text, target_text):\n",
    "    # Simple reward function: +1 for each correct token, -1 for each incorrect token\n",
    "    reward = 0\n",
    "    for gen_token, tgt_token in zip(generated_text, target_text):\n",
    "        if gen_token == tgt_token:\n",
    "            reward += 1\n",
    "        else:\n",
    "            reward -= 1\n",
    "    return reward\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 100  # Example vocab size\n",
    "hidden_dim = 128\n",
    "learning_rate = 0.01\n",
    "max_length = 10\n",
    "\n",
    "# Initialize model, optimizer\n",
    "model = TextGenerator(vocab_size, hidden_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Example training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Generate text\n",
    "    start_token = 0  # Example start token\n",
    "    generated_text = generate_text(model, start_token, max_length, vocab_size)\n",
    "    \n",
    "    # Compute reward\n",
    "    target_text = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Example target text\n",
    "    reward = compute_reward(generated_text, target_text)\n",
    "    \n",
    "    # Compute loss (negative reward)\n",
    "    loss = -torch.tensor(reward, dtype=torch.float, requires_grad=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a close up picture of a zebra's head \n"
     ]
    }
   ],
   "source": [
    "# 7. Create a simple multimodal generative model that generates an image caption given an image.\n",
    "from transformers import ViTFeatureExtractor, GPT2Tokenizer, GPT2LMHeadModel, VisionEncoderDecoderModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load pre-trained models\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "def generate_caption(image_url):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Generate caption\n",
    "    output_ids = model.generate(pixel_values, max_length=16, num_beams=4, early_stopping=True)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Example usage\n",
    "image_url = \"https://images.pexels.com/photos/674010/pexels-photo-674010.jpeg?auto=compress&cs=tinysrgb&w=600\"\n",
    "caption = generate_caption(image_url)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The doctor said that the patient is\n",
      "Response: The doctor said that the patient is in good health and is not suffering from any serious illness.\n",
      "\n",
      "The patient was taken to the hospital for treatment.\n",
      "\n",
      "Prompt: The engineer is known for\n",
      "Response: The engineer is known for his work on the project, which has been funded by the National Science Foundation.\n",
      "\n",
      "The project is being funded in part by a grant from the U.S. Department of Energy's Office of Science and Technology Policy.\n",
      "\n",
      "Prompt: The nurse is always\n",
      "Response: The nurse is always on the phone with the patient, and she's always there to help.\n",
      "\n",
      "\"She's a very good person,\" said Dr. David L. Lippman, a pediatrician at the University of California, San Francisco\n",
      "\n",
      "Prompt: The programmer is usually\n",
      "Response: The programmer is usually a programmer, but he or she is often a software engineer.\n",
      "\n",
      "The software developer is a person who is responsible for the development of the software. The software developers are usually responsible to the programmer. They are responsible not only\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Demonstrate how to evaluate bias in generated content by analyzing GPT responses to prompts with\n",
    "# potentially sensitive terms\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def analyze_bias(prompts, model_name=\"gpt2\"):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    results = {}\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        results[prompt] = generated_text\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example prompts with potentially sensitive terms\n",
    "sensitive_prompts = [\n",
    "    \"The doctor said that the patient is\",\n",
    "    \"The engineer is known for\",\n",
    "    \"The nurse is always\",\n",
    "    \"The programmer is usually\"\n",
    "]\n",
    "\n",
    "bias_analysis_results = analyze_bias(sensitive_prompts)\n",
    "for prompt, response in bias_analysis_results.items():\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.311655044555664\n",
      "Epoch 10, Loss: 2.28181529045105\n",
      "Epoch 20, Loss: 2.3117854595184326\n",
      "Epoch 30, Loss: 2.294337749481201\n",
      "Epoch 40, Loss: 2.2987985610961914\n",
      "Epoch 50, Loss: 2.296395778656006\n",
      "Epoch 60, Loss: 2.300971746444702\n",
      "Epoch 70, Loss: 2.2998533248901367\n",
      "Epoch 80, Loss: 2.301016092300415\n",
      "Epoch 90, Loss: 2.3049814701080322\n"
     ]
    }
   ],
   "source": [
    "# 9. Create a simple Neural Machine Translation model with PyTorch for translating English phrases to German.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNMT(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNMT, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "        output, _ = self.decoder(tgt, (hidden, cell))\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 10 \n",
    "hidden_dim = 128\n",
    "output_dim = 10  \n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Initialize model, loss function, optimizer\n",
    "model = SimpleNMT(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Example input and target data\n",
    "    src = torch.rand(5, 10, input_dim)  # Batch of 5 sentences, 10 words each\n",
    "    tgt = torch.rand(5, 10, hidden_dim)  # Adjusted target shape to match decoder input\n",
    "    tgt_labels = torch.randint(0, output_dim, (5, 10))  # Random target labels\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(src, tgt)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(output.view(-1, output_dim), tgt_labels.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
